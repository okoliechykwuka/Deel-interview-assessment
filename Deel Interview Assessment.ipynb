{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9677085a",
   "metadata": {},
   "source": [
    "### Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8ff4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "##torch and embedding libaries\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c050191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc90c9f2",
   "metadata": {},
   "source": [
    "#### Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2de21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df = pd.read_csv('transactions.csv')\n",
    "user_df = pd.read_csv('users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a1bbdd",
   "metadata": {},
   "source": [
    "#### Clean description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd3b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean description column\n",
    "def clean_data(df):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Pandas dataframe with of transactions made by users .\n",
    "    Returns:\n",
    "        df: returns a clean version of the dataframe\n",
    "    \"\"\"\n",
    "    df['description'] = df['description'].str.strip().str.lower()#.str.replace('[^\\w\\s]', '')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345656a0",
   "metadata": {},
   "source": [
    "### Task 1 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc3a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_match_metric(df, name):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Pandas dataframe with of transactions made by users .\n",
    "        name (str): name of users\n",
    "    Returns:\n",
    "        dict: dict contains user's id and matched metrics ranked based on metric_score\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare response data\n",
    "    response_data = {}\n",
    "    matched_transactions = []\n",
    "\n",
    "    # Calculate match metric for each transaction\n",
    "    for index, transaction in df.iterrows():\n",
    "        name= name\n",
    "        transaction_id = transaction['id']\n",
    "        transaction_description = transaction['description']\n",
    "\n",
    "        # Initialize match metric\n",
    "        match_metric = 0\n",
    "\n",
    "        # Perform fuzzy matching to calculate match metric\n",
    "        match_metric = fuzz.token_sort_ratio(name, transaction_description)\n",
    "        # Create a dictionary for the matched transaction\n",
    "        matched_transaction = {\n",
    "            'id': transaction_id,\n",
    "            'match_metric': match_metric\n",
    "        }\n",
    "\n",
    "        # Add the matched transaction to the list\n",
    "        matched_transactions.append(matched_transaction)\n",
    "\n",
    "    # Sort the matched transactions in descending order of match metric\n",
    "    matched_transactions = sorted(matched_transactions, key=lambda x: x['match_metric'], reverse=True)\n",
    "    # Add matched transactions and total number of matches to the response data\n",
    "    response_data['transactions'] = matched_transactions\n",
    "    response_data['total_number_of_matches'] = len(matched_transactions)\n",
    "    return response_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3bc98",
   "metadata": {},
   "source": [
    "#### Considered Edge Cases for Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2db426",
   "metadata": {},
   "source": [
    "Here's a list of some possible variations we might encounter:\n",
    "\n",
    "1. `name` is in the users dataframe/table.\n",
    "2. `name` is not of not of type `str`.\n",
    "3. when a dataframe is empty.\n",
    "\n",
    "\n",
    "Edge cases not taken care of\n",
    "\n",
    "1. `name` is not in the users dataframe/table.\n",
    "2. when we want to search for `multiple name` is the transaction table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b809a",
   "metadata": {},
   "source": [
    "### Task 2 Function\n",
    "\n",
    "Calculate embeddings for  all transactions that has similar descriptions to the input sting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "634b27eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def find_similar_transactions(df, input_text:str):\n",
    "    # Tokenize input text\n",
    "    input_tokens = tokenizer.tokenize(input_text)\n",
    "    total_tokens_used = len(input_tokens)\n",
    "\n",
    "    # Convert tokens to IDs and create input tensors\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "    input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "    # Get embeddings for input text\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    input_embedding = output[0].mean(dim=1).squeeze()\n",
    "\n",
    "    # Find similar transactions\n",
    "    similar_transactions = []\n",
    "    for _, transaction in df.iterrows():\n",
    "        description = transaction['description']\n",
    "        transaction_tokens = tokenizer.tokenize(description)\n",
    "        transaction_ids = tokenizer.convert_tokens_to_ids(transaction_tokens)\n",
    "        transaction_tensor = torch.tensor([transaction_ids])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(transaction_tensor)\n",
    "\n",
    "        transaction_embedding = output[0].mean(dim=1).squeeze()\n",
    "        similarity = torch.cosine_similarity(input_embedding, transaction_embedding, dim=0).item()\n",
    "\n",
    "        similar_transactions.append({'id': transaction['id'], 'embedding': transaction_embedding.tolist(), 'total_number_of_tokens_used': total_tokens_used, 'similarity': similarity})\n",
    "\n",
    "    # Sort transactions by similarity in descending order\n",
    "    similar_transactions.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    return similar_transactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4919564",
   "metadata": {},
   "source": [
    "### API end point for task 1 and Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e219d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: Optional[str] = None\n",
    "        \n",
    "class Description(BaseModel):\n",
    "    text: Optional[str] = None\n",
    "        \n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"EndPoint is working perfectly!\"}\n",
    "\n",
    "@app.post('/match_transactions')\n",
    "def match_transactions(name:User):\n",
    "\n",
    "    df = clean_data(trans_df)\n",
    "    response_data = calculate_match_metric(df, name)\n",
    "    #print(json.dumps(response_data))\n",
    "    # Return the response as JSON\n",
    "    return jsonable_encoder(response_data)\n",
    "\n",
    "\n",
    "        \n",
    "@app.post('/find_similar_transactions')\n",
    "def match_transactions_endpoint(text:Description):\n",
    "    df = clean_data(trans_df)\n",
    "    response_data = find_similar_transactions(df, text.text)\n",
    "#     print(response_data)\n",
    "    # Return the response as JSON\n",
    "    return jsonable_encoder(response_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b2c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [1752]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigate to this endpoint to test app: http://127.0.0.1:8000/docs#/default/\n",
      "INFO:     127.0.0.1:59617 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59617 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59619 - \"POST /find_similar_transactions HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "print('Navigate to this endpoint to test app: http://127.0.0.1:8000/docs#/default/')\n",
    "nest_asyncio.apply()\n",
    "\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a47372",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c0e0f",
   "metadata": {},
   "source": [
    "Given additional resources, suggest (in the README/pdf/keynote or otherwise), how you might take this proof of concept to production. Include any changes or improvements you might make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab88fb",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b8db5",
   "metadata": {},
   "source": [
    "Here are some suggestions for taking this proof of concept to production:\n",
    "\n",
    "**Improvements to current implementation:**\n",
    "\n",
    "1. Add more robust input validation, sanitization, error handling\n",
    "2. Containerize with Docker for easy deployment\n",
    "3. Add API keys, rate limiting, authentication\n",
    "4. Improve exception handling and logging\n",
    "5. Add automated tests (unit, integration)\n",
    "6. Enable CORS if front-end consuming the API is on different domain\n",
    "7. User a more efficient embedding function like Faiss or OpenAIEmbeddings\n",
    "8. Improve the run ime of the script\n",
    "\n",
    "\n",
    "**Scaling:**\n",
    "\n",
    "1. Switch to a production-grade database like PostgreSQL to store transaction/user data\n",
    "2. Add caching layer (Redis) to reduce load on database for common queries\n",
    "3. Optimize DB queries, add indexes for performance\n",
    "4. User Large Language Models and vector database to improve query search.\n",
    "\n",
    "\n",
    "**Monitoring:**\n",
    "\n",
    "1. Add performance monitoring (Prometheus) to track API latency, error rates\n",
    "2. Track usage metrics on all endpoints\n",
    "3. Set up logging aggregation (Elasticsearch) for analyzing logs\n",
    "\n",
    "**Model serving:**\n",
    "\n",
    "1. Serve sentence embedding model on dedicated model server (Seldon Core)\n",
    "2. Implement A/B testing for model variations\n",
    "3. Build pipeline for retraining model on new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bea2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266bb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy\n",
    "# !pip install python-Levenshtein\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
